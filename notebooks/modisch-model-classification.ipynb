{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - Data Prep for Classification Model\n",
    "\n",
    "This notebook downloads the **Fashion-MNIST** dataset and reorganizes it from its 10 default classes into our project's 8 classes.\n",
    "\n",
    "**Our Target Classes:**\n",
    "* `jacket`\n",
    "* `shirt`\n",
    "* `pants`\n",
    "* `shorts`\n",
    "* `skirt`\n",
    "* `dress`\n",
    "* `shoe`\n",
    "* `slipper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "249fbdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls\n",
      "Data dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data\n",
      "Final Dataset dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch\n",
      "Kaggle Clothes source dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "Kaggle Shoes source dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# Get project root\n",
    "CWD = pathlib.Path.cwd().resolve()\n",
    "PROJECT_ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# Final organized dataset directory (for YOLO training)\n",
    "DATASET_DIR = DATA_DIR / \"dataset-fashion-modisch\"\n",
    "TRAIN_DIR = DATASET_DIR / \"train\"\n",
    "VAL_DIR = DATASET_DIR / \"val\"\n",
    "\n",
    "# Source Kaggle dataset paths\n",
    "CLOTHES_DATASET_SLUG = \"ryanbadai/clothes-dataset\"\n",
    "SHOE_DATASET_SLUG = \"noobyogi0100/shoe-dataset\"\n",
    "\n",
    "CLOTHES_ZIP = DATA_DIR / \"clothes-dataset.zip\"\n",
    "SHOES_ZIP = DATA_DIR / \"shoe-dataset.zip\"\n",
    "\n",
    "CLOTHES_SOURCE_DIR = DATA_DIR / \"clothes-dataset-unzipped\"\n",
    "SHOES_SOURCE_DIR = DATA_DIR / \"shoe-dataset-unzipped\"\n",
    "\n",
    "# Add src to path\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Final Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"Kaggle Clothes source dir: {CLOTHES_SOURCE_DIR}\")\n",
    "print(f\"Kaggle Shoes source dir: {SHOES_SOURCE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af771e71",
   "metadata": {},
   "source": [
    "## Step 1: Download the Data\n",
    "\n",
    "We'll use the Ultralytics downloader to fetch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c2f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/macm4/.kaggle/kaggle.json'\n",
      "Authenticating\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/macm4/.kaggle/kaggle.json'\n",
      "Authentication successful.\n",
      "Dataset URL: https://www.kaggle.com/datasets/ryanbadai/clothes-dataset\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset.zip\n",
      "Downloading noobyogi0100/shoe-dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/noobyogi0100/shoe-dataset\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset.zip\n",
      "Downloading noobyogi0100/shoe-dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/noobyogi0100/shoe-dataset\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset.zip\n",
      "Unzipping clothes-dataset.zip...\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset.zip\n",
      "Unzipping clothes-dataset.zip...\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "Unzipping shoe-dataset.zip...\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "Unzipping shoe-dataset.zip...\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Authenticate and Download ---\n",
    "print(\"Authenticating\")\n",
    "try:\n",
    "    kaggle.api.authenticate()\n",
    "    print(\"Authentication successful.\")\n",
    "\n",
    "    # --- Download Clothes Dataset ---\n",
    "    if not CLOTHES_ZIP.exists():\n",
    "        kaggle.api.dataset_download_files(\n",
    "            CLOTHES_DATASET_SLUG,\n",
    "            path=DATA_DIR,\n",
    "            unzip=False  # Download as zip\n",
    "        )\n",
    "        # The Kaggle API often names the file after the dataset slug\n",
    "        # We must rename it to match our CLOTHES_ZIP path\n",
    "        downloaded_zip = DATA_DIR / f\"{CLOTHES_DATASET_SLUG.split('/')[-1]}.zip\"\n",
    "        if downloaded_zip.exists() and not CLOTHES_ZIP.exists():\n",
    "             downloaded_zip.rename(CLOTHES_ZIP)\n",
    "        print(f\"Download complete: {CLOTHES_ZIP}\")\n",
    "    else:\n",
    "        print(f\"{CLOTHES_ZIP.name} already downloaded.\")\n",
    "\n",
    "    # --- Download Shoe Dataset ---\n",
    "    if not SHOES_ZIP.exists():\n",
    "        print(f\"Downloading {SHOE_DATASET_SLUG}...\")\n",
    "        kaggle.api.dataset_download_files(\n",
    "            SHOE_DATASET_SLUG,\n",
    "            path=DATA_DIR,\n",
    "            unzip=False  # Download as zip\n",
    "        )\n",
    "        # Rename this zip file as well\n",
    "        downloaded_zip = DATA_DIR / f\"{SHOE_DATASET_SLUG.split('/')[-1]}.zip\"\n",
    "        if downloaded_zip.exists() and not SHOES_ZIP.exists():\n",
    "             downloaded_zip.rename(SHOES_ZIP)\n",
    "        print(f\"Download complete: {SHOES_ZIP}\")\n",
    "    else:\n",
    "        print(f\"{SHOES_ZIP.name} already downloaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Unzip Clothes Dataset\n",
    "CLOTHES_SOURCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not any(CLOTHES_SOURCE_DIR.iterdir()) and CLOTHES_ZIP.exists():\n",
    "    print(f\"Unzipping {CLOTHES_ZIP.name}...\")\n",
    "    with zipfile.ZipFile(CLOTHES_ZIP, 'r') as zf:\n",
    "        zf.extractall(CLOTHES_SOURCE_DIR)\n",
    "    print(f\"Unzip complete to: {CLOTHES_SOURCE_DIR}\")\n",
    "elif any(CLOTHES_SOURCE_DIR.iterdir()):\n",
    "    print(f\"Clothes dataset already unzipped at: {CLOTHES_SOURCE_DIR}\")\n",
    "else:\n",
    "    print(f\"Could not find {CLOTHES_ZIP.name} to unzip.\")\n",
    "\n",
    "# Unzip Shoe Dataset\n",
    "SHOES_SOURCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not any(SHOES_SOURCE_DIR.iterdir()) and SHOES_ZIP.exists():\n",
    "    print(f\"Unzipping {SHOES_ZIP.name}...\")\n",
    "    with zipfile.ZipFile(SHOES_ZIP, 'r') as zf:\n",
    "        zf.extractall(SHOES_SOURCE_DIR)\n",
    "    print(f\"Unzip complete to: {SHOES_SOURCE_DIR}\")\n",
    "elif any(SHOES_SOURCE_DIR.iterdir()):\n",
    "    print(f\"Shoe dataset already unzipped at: {SHOES_SOURCE_DIR}\")\n",
    "else:\n",
    "    print(f\"Could not find {SHOES_ZIP.name} to unzip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7be39a",
   "metadata": {},
   "source": [
    "## Step 3: Create Your New Class Folders\n",
    "\n",
    "This creates the `train` and `val` directories, each containing your 8 target class folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f38e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new directory structure for 16 classes...\n",
      "New structure created.\n"
     ]
    }
   ],
   "source": [
    "# target class\n",
    "CLASSES_TO_CREATE = [\n",
    "    'boot', 'dress', 'pants', 'shirt', 'sneaker', 'flip-flop', 'loafer',\n",
    "    'short', 'skirt', 'slipper', 't-shirt', 'blazer', 'hoodie', 'jacket', 'sweater', 'polo'\n",
    "]\n",
    "\n",
    "print(f\"Creating new directory structure for {len(CLASSES_TO_CREATE)} classes...\")\n",
    "for split in [TRAIN_DIR, VAL_DIR]:\n",
    "    for cls_name in CLASSES_TO_CREATE:\n",
    "        os.makedirs(split / cls_name, exist_ok=True)\n",
    "\n",
    "print(\"New structure created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: \"Re-label\" by Moving Files\n",
    "\n",
    "This is the key step. We use terminal commands (`mv`) to move all images from the Fashion-MNIST folders (e.g., `T-shirt/top`) into your new folders (e.g., `shirt`).\n",
    "\n",
    "This works because we are in the `notebooks/` directory, so `../fashion-mnist` points to the unzipped folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc46a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping new Kaggle datasets and splitting into train/val...\n",
      "\n",
      "Processing Clothes Dataset from: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "  [Warning] Source folder not found: Blazer\n",
      "  [Warning] Source folder not found: Celana_Panjang\n",
      "  [Warning] Source folder not found: Celana_Pendek\n",
      "  [Warning] Source folder not found: Gaun\n",
      "  [Warning] Source folder not found: Hoodie\n",
      "  [Warning] Source folder not found: Jaket\n",
      "  [Warning] Source folder not found: Jaket_Denim\n",
      "  [Warning] Source folder not found: Jaket_Olahraga\n",
      "  [Warning] Source folder not found: Jeans\n",
      "  [Warning] Source folder not found: Kaos\n",
      "  [Warning] Source folder not found: Kemeja\n",
      "  [Warning] Source folder not found: Mantel\n",
      "  [Warning] Source folder not found: Polo\n",
      "  [Warning] Source folder not found: Rok\n",
      "  [Warning] Source folder not found: Sweter\n",
      "\n",
      "Processing Shoe Dataset from: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n",
      "  [Warning] Source folder not found: boots\n",
      "  [Warning] Source folder not found: sneakers\n",
      "  [Warning] Source folder not found: flip flops\n",
      "  [Warning] Source folder not found: loafers\n",
      "  [Warning] Source folder not found: sandals\n",
      "  [Warning] Source folder not found: soccer shoes\n",
      "\n",
      "--- File Move Complete ---\n",
      "Total Train Images: 0\n",
      "Total Val Images: 0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Mapping new Kaggle datasets and splitting into train/val...\")\n",
    "\n",
    "# --- Mappings from Kaggle source folders to your 16 target class names ---\n",
    "\n",
    "# Target classes (as defined in the cell above)\n",
    "TARGET_CLASSES = set(CLASSES_TO_CREATE)\n",
    "\n",
    "# 1. ryanbadai/clothes-dataset\n",
    "CLOTHES_BASE_DIR = CLOTHES_SOURCE_DIR\n",
    "\n",
    "CLOTHES_MAP = {\n",
    "    \"Blazer\": \"blazer\",\n",
    "    \"Celana_Panjang\": \"pants\",\n",
    "    \"Celana_Pendek\": \"short\",\n",
    "    \"Gaun\": \"dress\",\n",
    "    \"Hoodie\": \"hoodie\",\n",
    "    \"Jaket\": \"jacket\",\n",
    "    \"Jaket_Denim\": \"jacket\",       # Grouped into 'jacket'\n",
    "    \"Jaket_Olahraga\": \"jacket\",    # Grouped into 'jacket'\n",
    "    \"Jeans\": \"pants\",          # Grouped into 'pants'\n",
    "    \"Kaos\": \"t-shirt\",\n",
    "    \"Kemeja\": \"shirt\",\n",
    "    \"Mantel\": \"jacket\",        # Grouped into 'jacket'\n",
    "    \"Polo\": \"polo\",\n",
    "    \"Rok\": \"skirt\",\n",
    "    \"Sweter\": \"sweater\",\n",
    "}\n",
    "\n",
    "# 2. noobyogi0100/shoe-dataset\n",
    "SHOES_BASE_DIR = SHOES_SOURCE_DIR\n",
    "\n",
    "SHOES_MAP = {\n",
    "    \"boots\": \"boot\",\n",
    "    \"sneakers\": \"sneaker\",\n",
    "    \"flip flops\": \"flip-flop\",\n",
    "    \"loafers\": \"loafer\",\n",
    "    \"sandals\": \"slipper\",      # Mapped to 'slipper'\n",
    "    \"soccer shoes\": \"sneaker\",     # Grouped into 'sneaker'\n",
    "}\n",
    "# --- End Mappings ---\n",
    "\n",
    "\n",
    "def split_and_copy_files(source_class_dir, target_class_name, train_dir, val_dir, split_ratio=0.8):\n",
    "    \"\"\"Copies files from source dir to train/val dirs with a split.\"\"\"\n",
    "    \n",
    "    if target_class_name not in TARGET_CLASSES:\n",
    "        print(f\"  [Skipping] Source '{source_class_dir.name}' maps to '{target_class_name}', which is not in TARGET_CLASSES.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Find all images (jpg, png, jpeg)\n",
    "    images = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "        images.extend(glob.glob(str(source_class_dir / ext)))\n",
    "    \n",
    "    if not images:\n",
    "        print(f\"  [Warning] No images found in {source_class_dir}\")\n",
    "        return 0, 0\n",
    "\n",
    "    random.seed(42) # for reproducible splits\n",
    "    random.shuffle(images)\n",
    "    split_point = int(len(images) * split_ratio)\n",
    "    train_files = images[:split_point]\n",
    "    val_files = images[split_point:]\n",
    "\n",
    "    # Get target directories\n",
    "    target_train_dir = train_dir / target_class_name\n",
    "    target_val_dir = val_dir / target_class_name\n",
    "\n",
    "    # Copy files\n",
    "    for f in train_files:\n",
    "        shutil.copy(f, target_train_dir / Path(f).name)\n",
    "    for f in val_files:\n",
    "        shutil.copy(f, target_val_dir / Path(f).name)\n",
    "        \n",
    "    return len(train_files), len(val_files)\n",
    "\n",
    "total_train = 0\n",
    "total_val = 0\n",
    "\n",
    "# --- Process Clothes Dataset ---\n",
    "print(f\"\\nProcessing Clothes Dataset from: {CLOTHES_BASE_DIR}\")\n",
    "for source_name, target_name in CLOTHES_MAP.items():\n",
    "    source_dir = CLOTHES_BASE_DIR / source_name\n",
    "    # Check for nested \"Clothes_Dataset\" folder\n",
    "    if not source_dir.exists():\n",
    "        nested_dir = CLOTHES_BASE_DIR / \"Clothes_Dataset\" / source_name\n",
    "        if nested_dir.exists():\n",
    "            source_dir = nested_dir\n",
    "        else:\n",
    "            print(f\"  [Warning] Source folder not found: {source_dir.name}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"Processing '{source_name}' -> '{target_name}'...\")\n",
    "    n_train, n_val = split_and_copy_files(source_dir, target_name, TRAIN_DIR, VAL_DIR)\n",
    "    print(f\"  Copied {n_train} train, {n_val} val files.\")\n",
    "    total_train += n_train\n",
    "    total_val += n_val\n",
    "\n",
    "# --- Process Shoe Dataset ---\n",
    "print(f\"\\nProcessing Shoe Dataset from: {SHOES_BASE_DIR}\")\n",
    "for source_name, target_name in SHOES_MAP.items():\n",
    "    source_dir = SHOES_BASE_DIR / source_name\n",
    "    if not source_dir.exists():\n",
    "        print(f\"  [Warning] Source folder not found: {source_dir.name}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing '{source_name}' -> '{target_name}'...\")\n",
    "    n_train, n_val = split_and_copy_files(source_dir, target_name, TRAIN_DIR, VAL_DIR)\n",
    "    print(f\"  Copied {n_train} train, {n_val} val files.\")\n",
    "    total_train += n_train\n",
    "    total_val += n_val\n",
    "\n",
    "print(\"\\n--- File Move Complete ---\")\n",
    "print(f\"Total Train Images: {total_train}\")\n",
    "print(f\"Total Val Images: {total_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: IMPORTANT - Check for Missing Classes\n",
    "\n",
    "The Fashion-MNIST dataset **does not contain** images for `shorts`, `skirt`, or `slipper`.\n",
    "\n",
    "Your folders for these classes are **empty**! You will need to find images for these classes and add them to the `train/` and `val/` subfolders yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9a5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5d: Oversample imbalanced classes in TRAIN_DIR (skirt, short/shorts)\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "TRAIN_DIR = pathlib.Path(TRAIN_DIR)  # ensure Path\n",
    "random.seed(42)\n",
    "\n",
    "TARGETS = [\n",
    "    'boot', 'dress', 'pants', 'shirt', 'sneaker', 'flip-flop', 'loafer',\n",
    "    'short', 'skirt', 'slipper', 't-shirt', 'blazer', 'hoodie', 'sweater', 'polo'\n",
    "]\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "# Optional PIL-based augmentation\n",
    "try:\n",
    "    from PIL import Image, ImageEnhance, ImageOps\n",
    "    HAVE_PIL = True\n",
    "except Exception:\n",
    "    HAVE_PIL = False\n",
    "\n",
    "def _unique_path(dirpath: pathlib.Path, stem: str, ext: str) -> pathlib.Path:\n",
    "    i = 0\n",
    "    while True:\n",
    "        name = f\"{stem}_aug{'' if i==0 else f'_{i}'}{ext}\"\n",
    "        out = dirpath / name\n",
    "        if not out.exists():\n",
    "            return out\n",
    "        i += 1\n",
    "\n",
    "def _list_images(dirpath: pathlib.Path):\n",
    "    return [p for p in dirpath.glob(\"*\") if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]\n",
    "\n",
    "def _pil_augment(img: \"Image.Image\") -> \"Image.Image\":\n",
    "    # simple, fast, safe transforms\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageOps.mirror(img)\n",
    "    angle = random.uniform(-12, 12)\n",
    "    img = img.rotate(angle, resample=Image.BICUBIC, expand=False, fillcolor=(255, 255, 255))\n",
    "    # slight brightness/contrast jitter\n",
    "    img = ImageEnhance.Brightness(img).enhance(random.uniform(0.9, 1.1))\n",
    "    img = ImageEnhance.Contrast(img).enhance(random.uniform(0.9, 1.1))\n",
    "    return img\n",
    "\n",
    "def _augment_or_copy(src: pathlib.Path, dst: pathlib.Path):\n",
    "    if HAVE_PIL:\n",
    "        try:\n",
    "            with Image.open(src) as im:\n",
    "                if im.mode not in (\"RGB\", \"L\"):\n",
    "                    im = im.convert(\"RGB\")\n",
    "                # Save as same ext if supported; else default to .jpg\n",
    "                ext = dst.suffix.lower()\n",
    "                im = _pil_augment(im)\n",
    "                params = {}\n",
    "                if ext in {\".jpg\", \".jpeg\"}:\n",
    "                    params = {\"quality\": 90, \"optimize\": True}\n",
    "                im.save(dst, **params)\n",
    "                return\n",
    "        except Exception as _:\n",
    "            pass\n",
    "    # Fallback to raw copy\n",
    "    shutil.copy2(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fafece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train counts:\n",
      " - blazer    : 500\n",
      " - boot      : 249\n",
      " - dress     : 500\n",
      " - flip-flop : 249\n",
      " - hoodie    : 500\n",
      " - jacket    : 1995\n",
      " - loafer    : 249\n",
      " - pants     : 1000\n",
      " - polo      : 500\n",
      " - shirt     : 500\n",
      " - short     : 500\n",
      " - skirt     : 709\n",
      " - slipper   : 249\n",
      " - sneaker   : 478\n",
      " - sweater   : 500\n",
      " - t-shirt   : 500\n",
      "\n",
      "Oversampling to target count = 1995\n",
      " - boot: adding 1746 samples...\n",
      " - dress: adding 1495 samples...\n",
      " - pants: adding 995 samples...\n",
      " - shirt: adding 1495 samples...\n",
      " - sneaker: adding 1517 samples...\n",
      " - flip-flop: adding 1746 samples...\n",
      " - loafer: adding 1746 samples...\n",
      " - short: adding 1495 samples...\n",
      " - skirt: adding 1286 samples...\n",
      " - slipper: adding 1746 samples...\n",
      " - t-shirt: adding 1495 samples...\n",
      " - blazer: adding 1495 samples...\n",
      " - hoodie: adding 1495 samples...\n",
      " - sweater: adding 1495 samples...\n",
      " - polo: adding 1495 samples...\n",
      "\n",
      "Train counts after oversampling:\n",
      " - blazer    : 1995\n",
      " - boot      : 1995\n",
      " - dress     : 1995\n",
      " - flip-flop : 1995\n",
      " - hoodie    : 1995\n",
      " - jacket    : 1995\n",
      " - loafer    : 1995\n",
      " - pants     : 1995\n",
      " - polo      : 1995\n",
      " - shirt     : 1995\n",
      " - short     : 1995\n",
      " - skirt     : 1995\n",
      " - slipper   : 1995\n",
      " - sneaker   : 1995\n",
      " - sweater   : 1995\n",
      " - t-shirt   : 1995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count current train images per class (based on actual folders)\n",
    "class_dirs = [d for d in TRAIN_DIR.iterdir() if d.is_dir()]\n",
    "counts = {d.name: len(_list_images(d)) for d in class_dirs}\n",
    "print(\"Current train counts:\")\n",
    "for k in sorted(counts):\n",
    "    print(f\" - {k:10s}: {counts[k]}\")\n",
    "\n",
    "if not counts:\n",
    "    raise RuntimeError(\"No training classes found in TRAIN_DIR.\")\n",
    "\n",
    "target_count = max(counts.values())\n",
    "targets_existing = [c for c in TARGETS if (TRAIN_DIR / c).exists()]\n",
    "if not targets_existing:\n",
    "    print(\"No target classes (skirt/short/shorts) found in TRAIN_DIR. Skipping oversampling.\")\n",
    "else:\n",
    "    print(f\"\\nOversampling to target count = {target_count}\")\n",
    "    for cls in targets_existing:\n",
    "        cls_dir = TRAIN_DIR / cls\n",
    "        imgs = _list_images(cls_dir)\n",
    "        n = len(imgs)\n",
    "        if n == 0:\n",
    "            print(f\" - {cls}: no images to oversample from; add some first.\")\n",
    "            continue\n",
    "        need = target_count - n\n",
    "        if need <= 0:\n",
    "            print(f\" - {cls}: already >= target ({n} >= {target_count}).\")\n",
    "            continue\n",
    "\n",
    "        print(f\" - {cls}: adding {need} samples...\")\n",
    "        i = 0\n",
    "        while i < need:\n",
    "            src = random.choice(imgs)\n",
    "            stem = src.stem\n",
    "            ext = src.suffix.lower()\n",
    "            if ext not in ALLOWED_EXTS:\n",
    "                ext = \".jpg\"\n",
    "            dst = _unique_path(cls_dir, stem=stem, ext=ext)\n",
    "            _augment_or_copy(src, dst)\n",
    "            i += 1\n",
    "\n",
    "# Re-count after oversampling\n",
    "counts_after = {d.name: len(_list_images(d)) for d in class_dirs}\n",
    "print(\"\\nTrain counts after oversampling:\")\n",
    "for k in sorted(counts_after):\n",
    "    print(f\" - {k:10s}: {counts_after[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade6a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (train):\n",
      " - boot    : 1636\n",
      " - dress   : 1596\n",
      " - pants   : 1596\n",
      " - shirt   : 1596\n",
      " - sneaker : 1673\n",
      " - flip-flop: 1636\n",
      " - loafer  : 1636\n",
      " - short   : 1596\n",
      " - skirt   : 1596\n",
      " - slipper : 1636\n",
      " - t-shirt : 1596\n",
      " - blazer  : 1596\n",
      " - hoodie  : 1596\n",
      " - jacket  : 1596\n",
      " - sweater : 1596\n",
      " - polo    : 1596\n",
      "\n",
      "Class counts (val):\n",
      " - boot    : 409\n",
      " - dress   : 399\n",
      " - pants   : 399\n",
      " - shirt   : 399\n",
      " - sneaker : 418\n",
      " - flip-flop: 409\n",
      " - loafer  : 409\n",
      " - short   : 399\n",
      " - skirt   : 399\n",
      " - slipper : 409\n",
      " - t-shirt : 399\n",
      " - blazer  : 399\n",
      " - hoodie  : 399\n",
      " - jacket  : 399\n",
      " - sweater : 399\n",
      " - polo    : 399\n"
     ]
    }
   ],
   "source": [
    "# Count per-class files in train/ and val/\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_files(dirpath: pathlib.Path) -> dict:\n",
    "    counts = {}\n",
    "    for cls in CLASSES_TO_CREATE:\n",
    "        d = dirpath / cls\n",
    "        n = len([f for f in d.glob(\"*\") if f.is_file()]) if d.exists() else 0\n",
    "        counts[cls] = n\n",
    "    return counts\n",
    "\n",
    "train_counts = count_files(TRAIN_DIR)\n",
    "val_counts = count_files(VAL_DIR)\n",
    "\n",
    "print(\"Class counts (train):\")\n",
    "for k, v in train_counts.items():\n",
    "    print(f\" - {k:8s}: {v}\")\n",
    "\n",
    "print(\"\\nClass counts (val):\")\n",
    "for k, v in val_counts.items():\n",
    "    print(f\" - {k:8s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5de23",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d497f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n",
      "Val classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset sanity print: list classes in train/ and val/ (no syncing/moving) ---\n",
    "from pathlib import Path\n",
    "\n",
    "train_dir = TRAIN_DIR\n",
    "val_dir = VAL_DIR\n",
    "\n",
    "train_classes = sorted([d.name for d in Path(train_dir).iterdir() if d.is_dir()])\n",
    "val_classes = sorted([d.name for d in Path(val_dir).iterdir() if d.is_dir()])\n",
    "\n",
    "print(\"Train classes:\", train_classes)\n",
    "print(\"Val classes:\", val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0736f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n",
      "Fixed empty splits: none\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "def list_images(dirpath: Path):\n",
    "    return [p for p in dirpath.glob(\"*\") if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]\n",
    "\n",
    "train_dir = Path(TRAIN_DIR)\n",
    "val_dir = Path(VAL_DIR)\n",
    "\n",
    "train_classes = {d.name for d in train_dir.iterdir() if d.is_dir()}\n",
    "val_classes = {d.name for d in val_dir.iterdir() if d.is_dir()}\n",
    "all_classes = sorted(train_classes | val_classes)\n",
    "\n",
    "# Make sure each class exists in both splits\n",
    "for cls in all_classes:\n",
    "    (train_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure at least 1 image per class per split by borrowing from the other split if needed\n",
    "fixed = []\n",
    "for cls in all_classes:\n",
    "    tr_imgs = list_images(train_dir / cls)\n",
    "    va_imgs = list_images(val_dir / cls)\n",
    "\n",
    "    if len(tr_imgs) == 0 and len(va_imgs) > 0:\n",
    "        src = random.choice(va_imgs)\n",
    "        dst = (train_dir / cls) / src.name\n",
    "        c = 1\n",
    "        while dst.exists():\n",
    "            dst = dst.with_name(f\"{src.stem}_{c}{src.suffix}\")\n",
    "            c += 1\n",
    "        shutil.copy2(src, dst)\n",
    "        fixed.append(f\"train/{cls}\")\n",
    "\n",
    "    if len(va_imgs) == 0 and len(tr_imgs) > 0:\n",
    "        src = random.choice(tr_imgs)\n",
    "        dst = (val_dir / cls) / src.name\n",
    "        c = 1\n",
    "        while dst.exists():\n",
    "            dst = dst.with_name(f\"{src.stem}_{c}{src.suffix}\")\n",
    "            c += 1\n",
    "        shutil.copy2(src, dst)\n",
    "        fixed.append(f\"val/{cls}\")\n",
    "\n",
    "print(\"Classes:\", all_classes)\n",
    "print(\"Fixed empty splits:\", fixed if fixed else \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0246acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-epoch reporting WITHOUT touching DEFAULT_CFG (prevents YAML RepresenterError) ---\n",
    "from pathlib import Path\n",
    "\n",
    "REPORT_CSV = PROJECT_ROOT / \"runs-cls\" / \"epoch_report.csv\"\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    csv_path = Path(trainer.save_dir) / \"results.csv\"\n",
    "    if not csv_path.exists():\n",
    "        return\n",
    "    last = csv_path.read_text().strip().splitlines()[-1]\n",
    "    print(f\"[epoch {trainer.epoch + 1}] {last}\")\n",
    "    if not REPORT_CSV.exists():\n",
    "        header = csv_path.read_text().splitlines()[0]\n",
    "        REPORT_CSV.write_text(header + \"\\n\")\n",
    "    with REPORT_CSV.open(\"a\") as f:\n",
    "        f.write(last + \"\\n\")\n",
    "\n",
    "def register_callbacks(model):\n",
    "    # avoid duplicate registration if re-running cells\n",
    "    try:\n",
    "        model.remove_callback(\"on_fit_epoch_end\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.add_callback(\"on_fit_epoch_end\", on_fit_epoch_end)\n",
    "    print(\"Per-epoch report callback registered. Writing mirror CSV to:\", REPORT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a263e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 1.6MB/s 3.4s.3s<0.0s2s8.8ss\n",
      "Per-epoch report callback registered. Writing mirror CSV to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/epoch_report.csv\n",
      "Ultralytics 8.3.223 üöÄ Python-3.11.14 torch-2.9.0 MPS (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n-cls-fashion, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... found 25772 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... found 6443 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=16\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    350736  ultralytics.nn.modules.head.Classify         [256, 16]                     \n",
      "YOLOv8n-cls summary: 56 layers, 1,458,784 parameters, 1,458,784 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 373.9¬±56.6 MB/s, size: 109.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... 25772 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25772/25772 5.7Kit/s 4.5s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train/boot/image117.jpeg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 452.5¬±80.5 MB/s, size: 112.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... 6443 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6443/6443 6.6Kit/s 1.0s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val/slipper/image288.jpeg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       1/30      1.11G      1.923         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:55<1.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.3sss\n",
      "                   all       0.68      0.966\n",
      "[epoch 1] 1,417.59,1.92332,0.68043,0.96616,1.00045,0.00332506,0.00332506,0.00332506\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       2/30      2.12G     0.9254         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:54<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.793      0.983\n",
      "[epoch 2] 2,834.239,0.92545,0.79311,0.98293,0.6287,0.00643867,0.00643867,0.00643867\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       3/30      1.12G     0.6459         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:51<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.828      0.992\n",
      "[epoch 3] 3,1307.49,0.64594,0.82772,0.99239,0.48668,0.00933227,0.00933227,0.00933227\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       4/30      2.11G     0.5072         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.9it/s 7:43<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.865      0.993\n",
      "[epoch 4] 4,1833.43,0.50724,0.86512,0.99317,0.39393,0.00901,0.00901,0.00901\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       5/30      2.12G     0.4008         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:16<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.898      0.996\n",
      "[epoch 5] 5,2272.76,0.40076,0.89787,0.99643,0.30091,0.00868,0.00868,0.00868\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       6/30      2.12G     0.3371         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:57<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.906      0.997\n",
      "[epoch 6] 6,2693.58,0.33712,0.90579,0.99659,0.2867,0.00835,0.00835,0.00835\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       7/30      2.12G     0.2966         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:52<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.9it/s 58.9s1.2ss\n",
      "                   all      0.899      0.996\n",
      "[epoch 7] 7,3104.7,0.29661,0.89912,0.99596,0.30596,0.00802,0.00802,0.00802\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       8/30      2.12G       0.26         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:59<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.917      0.997\n",
      "[epoch 8] 8,3587.5,0.25999,0.91681,0.99736,0.2554,0.00769,0.00769,0.00769\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       9/30      2.12G     0.2365         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:31<0.9s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:001.2sss\n",
      "                   all      0.918      0.998\n",
      "[epoch 9] 9,4039.2,0.23651,0.91774,0.99752,0.25006,0.00736,0.00736,0.00736\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      10/30      2.12G     0.2156         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 7:01<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.1sss\n",
      "                   all      0.931      0.998\n",
      "[epoch 10] 10,4523.32,0.21559,0.93093,0.99798,0.21408,0.00703,0.00703,0.00703\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      11/30      2.12G     0.2079         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:33<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:051.2sss\n",
      "                   all      0.937      0.999\n",
      "[epoch 11] 11,4981.78,0.20789,0.93699,0.9986,0.19803,0.0067,0.0067,0.0067\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      12/30      2.12G     0.1891         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:53<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:011.2sss\n",
      "                   all      0.951      0.999\n",
      "[epoch 12] 12,5396.06,0.18906,0.95064,0.99876,0.15965,0.00637,0.00637,0.00637\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      13/30      2.11G     0.1724         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:57<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.7it/s 1:121.3sss\n",
      "                   all       0.95      0.998\n",
      "[epoch 13] 13,5826.07,0.17239,0.94956,0.99798,0.17469,0.00604,0.00604,0.00604\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      14/30      2.12G      0.167         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:11<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.956      0.998\n",
      "[epoch 14] 14,6259.67,0.16696,0.95592,0.99814,0.1443,0.00571,0.00571,0.00571\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      15/30      2.12G      0.151         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.2it/s 5:46<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.956      0.999\n",
      "[epoch 15] 15,6668.22,0.15096,0.95577,0.9986,0.14368,0.00538,0.00538,0.00538\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      16/30      2.12G     0.1424         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:51<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:051.2sss\n",
      "                   all      0.956      0.999\n",
      "[epoch 16] 16,7144.7,0.14244,0.95608,0.99907,0.14841,0.00505,0.00505,0.00505\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      17/30      2.12G      0.138         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:30<1.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.6it/s 1:301.8sss\n",
      "                   all      0.962      0.999\n",
      "[epoch 17] 17,7625.75,0.13798,0.96197,0.99891,0.1296,0.00472,0.00472,0.00472\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      18/30      2.11G     0.1294         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:47<0.9s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.7it/s 1:081.9sss\n",
      "                   all      0.962      0.999\n",
      "[epoch 18] 18,8102.08,0.12938,0.96244,0.99922,0.12479,0.00439,0.00439,0.00439\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      19/30      2.12G     0.1187         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:17<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.964      0.999\n",
      "[epoch 19] 19,8542.49,0.11866,0.96399,0.99876,0.12334,0.00406,0.00406,0.00406\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      20/30      2.11G     0.1139         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:57<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.965      0.999\n",
      "[epoch 20] 20,8961.73,0.11394,0.96492,0.99907,0.11871,0.00373,0.00373,0.00373\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      21/30      2.11G     0.1054         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.8it/s 8:00<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.964      0.999\n",
      "[epoch 21] 21,9504.29,0.10538,0.9643,0.99938,0.11956,0.0034,0.0034,0.0034\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      22/30      2.11G    0.09886         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:28<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.3sss\n",
      "                   all      0.967      0.999\n",
      "[epoch 22] 22,9956.06,0.09886,0.96694,0.99922,0.1179,0.00307,0.00307,0.00307\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      23/30      2.12G    0.09531         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.8it/s 8:18<1.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.6it/s 1:311.1sss\n",
      "                   all      0.967      0.999\n",
      "[epoch 23] 23,10546.1,0.09531,0.9671,0.99922,0.11103,0.00274,0.00274,0.00274\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      24/30      2.12G    0.09017         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:26<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.5it/s 1:331.8sss\n",
      "                   all      0.966      0.999\n",
      "[epoch 24] 24,11025.9,0.09017,0.96554,0.99907,0.10563,0.00241,0.00241,0.00241\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      25/30      2.12G    0.08785         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:38<1.2s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.6it/s 1:291.7sss\n",
      "                   all      0.968      0.999\n",
      "[epoch 25] 25,11513.7,0.08785,0.96803,0.99907,0.10498,0.00208,0.00208,0.00208\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      26/30      2.12G    0.08333         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.9it/s 7:05<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.968      0.999\n",
      "[epoch 26] 26,12001.7,0.08333,0.96849,0.99907,0.10469,0.00175,0.00175,0.00175\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      27/30      2.11G    0.08012         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:56<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:071.6sss\n",
      "                   all      0.968      0.999\n",
      "[epoch 27] 27,12485.2,0.08012,0.96834,0.99922,0.10198,0.00142,0.00142,0.00142\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      28/30      2.11G    0.07533         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.9it/s 7:36<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:051.2sss\n",
      "                   all      0.969      0.999\n",
      "[epoch 28] 28,13006.5,0.07533,0.9688,0.99922,0.10023,0.00109,0.00109,0.00109\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      29/30      2.11G    0.07337         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:08<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.3sss\n",
      "                   all       0.97      0.999\n",
      "[epoch 29] 29,13437.7,0.07337,0.9702,0.99907,0.09796,0.00076,0.00076,0.00076\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      30/30      2.12G    0.07053         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:02<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:071.7sss\n",
      "                   all      0.969      0.999\n",
      "[epoch 30] 30,13867.7,0.07053,0.96896,0.99891,0.09895,0.00043,0.00043,0.00043\n",
      "\n",
      "30 epochs completed in 3.852 hours.\n",
      "Optimizer stripped from /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/last.pt, 3.0MB\n",
      "Optimizer stripped from /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt...\n",
      "Ultralytics 8.3.223 üöÄ Python-3.11.14 torch-2.9.0 MPS (Apple M4)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,455,376 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... found 25772 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... found 6443 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:061.2sss\n",
      "                   all       0.97      0.999\n",
      "Speed: 0.1ms preprocess, 0.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "[epoch 30] 30,13867.7,0.07053,0.96896,0.99891,0.09895,0.00043,0.00043,0.00043\n",
      "Best weights: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Train YOLOv8n-cls (MPS if available) ---\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Remove any default callbacks so the 'callbacks' key doesn't leak into args.yaml/cfg\n",
    "try:\n",
    "    if hasattr(DEFAULT_CFG, \"callbacks\"):\n",
    "        delattr(DEFAULT_CFG, \"callbacks\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 4\n",
    "BATCH = 64\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "model = YOLO(\"yolov8n-cls.pt\")\n",
    "\n",
    "# Extra safety: strip any 'callbacks' from model overrides if present\n",
    "try:\n",
    "    if hasattr(model, \"overrides\") and isinstance(model.overrides, dict):\n",
    "        model.overrides.pop(\"callbacks\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Register console/CSV epoch reporter on the model only (not in DEFAULT_CFG)\n",
    "try:\n",
    "    register_callbacks(model)\n",
    "except NameError:\n",
    "    REPORT_CSV = PROJECT_ROOT / \"runs-cls\" / \"epoch_report.csv\"\n",
    "    def _on_fit_epoch_end(trainer):\n",
    "        csv_path = Path(trainer.save_dir) / \"results.csv\"\n",
    "        if not csv_path.exists():\n",
    "            return\n",
    "        last = csv_path.read_text().strip().splitlines()[-1]\n",
    "        print(f\"[epoch {trainer.epoch + 1}] {last}\")\n",
    "        if not REPORT_CSV.exists():\n",
    "            header = csv_path.read_text().splitlines()[0]\n",
    "            REPORT_CSV.write_text(header + \"\\n\")\n",
    "        with REPORT_CSV.open(\"a\") as f:\n",
    "            f.write(last + \"\\n\")\n",
    "    try:\n",
    "        model.remove_callback(\"on_fit_epoch_end\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.add_callback(\"on_fit_epoch_end\", _on_fit_epoch_end)\n",
    "    print(\"Per-epoch report callback registered. Writing mirror CSV to:\", REPORT_CSV)\n",
    "\n",
    "RUNS_DIR = PROJECT_ROOT / \"runs-cls\"\n",
    "NAME = \"yolov8n-cls-fashion\"\n",
    "EPOCHS = 30\n",
    "IMGSZ = 224\n",
    "SEED = 42\n",
    "\n",
    "results = model.train(\n",
    "    data=str(DATASET_DIR),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=NAME,\n",
    "    seed=SEED,\n",
    "    patience=10,         # early stopping\n",
    "    verbose=True,\n",
    "    device=DEVICE,\n",
    "    workers=NUM_WORKERS,\n",
    "    plots=False,\n",
    ")\n",
    "\n",
    "best_path = RUNS_DIR / NAME / \"weights\" / \"best.pt\"\n",
    "print(\"Best weights:\", best_path if best_path.exists() else \"see run dir for weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f476ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.223 üöÄ Python-3.11.14 torch-2.9.0 CPU (Apple M4)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,455,376 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... found 25772 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... found 6443 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 448.9¬±104.6 MB/s, size: 112.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... 6443 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6443/6443 11.2Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val/slipper/image288.jpeg: corrupt JPEG restored and saved\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 101/101 0.7it/s 2:161.3s\n",
      "                   all       0.97      0.999\n",
      "Speed: 0.0ms preprocess, 11.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion-val2\u001b[0m\n",
      "top1: 0.9700, top5: 0.9991\n",
      "Val plots saved to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion-val\n"
     ]
    }
   ],
   "source": [
    "metrics = model.val(\n",
    "    data=str(DATASET_DIR),\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    plots=True,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=NAME + \"-val\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"top1: {metrics.top1:.4f}, top5: {metrics.top5:.4f}\")\n",
    "except Exception:\n",
    "    print(\"Validation metrics:\", metrics)\n",
    "print(\"Val plots saved to:\", RUNS_DIR / (NAME + \"-val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8355b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion-pred2\u001b[0m\n",
      "Saved predictions to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion-pred\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR = PROJECT_ROOT / \"test\"\n",
    "\n",
    "preds = model.predict(\n",
    "    source=str(TEST_DIR),\n",
    "    imgsz=IMGSZ,\n",
    "    save=True,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=NAME + \"-pred\",\n",
    "    verbose=False,\n",
    ")\n",
    "print(\"Saved predictions to:\", RUNS_DIR / (NAME + \"-pred\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional exports for deployment\n",
    "try:\n",
    "    model.export(format=\"onnx\", opset=12)\n",
    "    model.export(format=\"torchscript\")\n",
    "    print(\"Exported ONNX and TorchScript.\")\n",
    "except Exception as e:\n",
    "    print(\"Export skipped:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
