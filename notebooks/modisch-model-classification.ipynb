{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - Data Prep for Classification Model\n",
    "\n",
    "This notebook downloads the **Fashion-MNIST** dataset and reorganizes it from its 10 default classes into our project's 8 classes.\n",
    "\n",
    "**Our Target Classes:**\n",
    "* `jacket`\n",
    "* `shirt`\n",
    "* `pants`\n",
    "* `shorts`\n",
    "* `skirt`\n",
    "* `dress`\n",
    "* `shoe`\n",
    "* `slipper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249fbdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls\n",
      "Data dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data\n",
      "Final Dataset dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch\n",
      "Kaggle Clothes source dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "Kaggle Shoes source dir: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# Get project root\n",
    "CWD = pathlib.Path.cwd().resolve()\n",
    "PROJECT_ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# Final organized dataset directory (for YOLO training)\n",
    "DATASET_DIR = DATA_DIR / \"dataset-fashion-modisch\"\n",
    "TRAIN_DIR = DATASET_DIR / \"train\"\n",
    "VAL_DIR = DATASET_DIR / \"val\"\n",
    "\n",
    "# Source Kaggle dataset paths\n",
    "CLOTHES_DATASET_SLUG = \"ryanbadai/clothes-dataset\"\n",
    "SHOE_DATASET_SLUG = \"noobyogi0100/shoe-dataset\"\n",
    "\n",
    "CLOTHES_ZIP = DATA_DIR / \"clothes-dataset.zip\"\n",
    "SHOES_ZIP = DATA_DIR / \"shoe-dataset.zip\"\n",
    "\n",
    "CLOTHES_SOURCE_DIR = DATA_DIR / \"clothes-dataset-unzipped\"\n",
    "SHOES_SOURCE_DIR = DATA_DIR / \"shoe-dataset-unzipped\"\n",
    "\n",
    "# Add src to path\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Final Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"Kaggle Clothes source dir: {CLOTHES_SOURCE_DIR}\")\n",
    "print(f\"Kaggle Shoes source dir: {SHOES_SOURCE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af771e71",
   "metadata": {},
   "source": [
    "## Step 1: Download the Data\n",
    "\n",
    "We'll use the Ultralytics downloader to fetch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c2f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/macm4/.kaggle/kaggle.json'\n",
      "Authenticating\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/macm4/.kaggle/kaggle.json'\n",
      "Authentication successful.\n",
      "Dataset URL: https://www.kaggle.com/datasets/ryanbadai/clothes-dataset\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset.zip\n",
      "Downloading noobyogi0100/shoe-dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/noobyogi0100/shoe-dataset\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset.zip\n",
      "Downloading noobyogi0100/shoe-dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/noobyogi0100/shoe-dataset\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset.zip\n",
      "Unzipping clothes-dataset.zip...\n",
      "Download complete: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset.zip\n",
      "Unzipping clothes-dataset.zip...\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "Unzipping shoe-dataset.zip...\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "Unzipping shoe-dataset.zip...\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n",
      "Unzip complete to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Authenticate and Download ---\n",
    "print(\"Authenticating\")\n",
    "try:\n",
    "    kaggle.api.authenticate()\n",
    "    print(\"Authentication successful.\")\n",
    "\n",
    "    # --- Download Clothes Dataset ---\n",
    "    if not CLOTHES_ZIP.exists():\n",
    "        kaggle.api.dataset_download_files(\n",
    "            CLOTHES_DATASET_SLUG,\n",
    "            path=DATA_DIR,\n",
    "            unzip=False  # Download as zip\n",
    "        )\n",
    "        # The Kaggle API often names the file after the dataset slug\n",
    "        # We must rename it to match our CLOTHES_ZIP path\n",
    "        downloaded_zip = DATA_DIR / f\"{CLOTHES_DATASET_SLUG.split('/')[-1]}.zip\"\n",
    "        if downloaded_zip.exists() and not CLOTHES_ZIP.exists():\n",
    "             downloaded_zip.rename(CLOTHES_ZIP)\n",
    "        print(f\"Download complete: {CLOTHES_ZIP}\")\n",
    "    else:\n",
    "        print(f\"{CLOTHES_ZIP.name} already downloaded.\")\n",
    "\n",
    "    # --- Download Shoe Dataset ---\n",
    "    if not SHOES_ZIP.exists():\n",
    "        print(f\"Downloading {SHOE_DATASET_SLUG}...\")\n",
    "        kaggle.api.dataset_download_files(\n",
    "            SHOE_DATASET_SLUG,\n",
    "            path=DATA_DIR,\n",
    "            unzip=False  # Download as zip\n",
    "        )\n",
    "        # Rename this zip file as well\n",
    "        downloaded_zip = DATA_DIR / f\"{SHOE_DATASET_SLUG.split('/')[-1]}.zip\"\n",
    "        if downloaded_zip.exists() and not SHOES_ZIP.exists():\n",
    "             downloaded_zip.rename(SHOES_ZIP)\n",
    "        print(f\"Download complete: {SHOES_ZIP}\")\n",
    "    else:\n",
    "        print(f\"{SHOES_ZIP.name} already downloaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Unzip Clothes Dataset\n",
    "CLOTHES_SOURCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not any(CLOTHES_SOURCE_DIR.iterdir()) and CLOTHES_ZIP.exists():\n",
    "    print(f\"Unzipping {CLOTHES_ZIP.name}...\")\n",
    "    with zipfile.ZipFile(CLOTHES_ZIP, 'r') as zf:\n",
    "        zf.extractall(CLOTHES_SOURCE_DIR)\n",
    "    print(f\"Unzip complete to: {CLOTHES_SOURCE_DIR}\")\n",
    "elif any(CLOTHES_SOURCE_DIR.iterdir()):\n",
    "    print(f\"Clothes dataset already unzipped at: {CLOTHES_SOURCE_DIR}\")\n",
    "else:\n",
    "    print(f\"Could not find {CLOTHES_ZIP.name} to unzip.\")\n",
    "\n",
    "# Unzip Shoe Dataset\n",
    "SHOES_SOURCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not any(SHOES_SOURCE_DIR.iterdir()) and SHOES_ZIP.exists():\n",
    "    print(f\"Unzipping {SHOES_ZIP.name}...\")\n",
    "    with zipfile.ZipFile(SHOES_ZIP, 'r') as zf:\n",
    "        zf.extractall(SHOES_SOURCE_DIR)\n",
    "    print(f\"Unzip complete to: {SHOES_SOURCE_DIR}\")\n",
    "elif any(SHOES_SOURCE_DIR.iterdir()):\n",
    "    print(f\"Shoe dataset already unzipped at: {SHOES_SOURCE_DIR}\")\n",
    "else:\n",
    "    print(f\"Could not find {SHOES_ZIP.name} to unzip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7be39a",
   "metadata": {},
   "source": [
    "## Step 3: Create Your New Class Folders\n",
    "\n",
    "This creates the `train` and `val` directories, each containing your 8 target class folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f38e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new directory structure for 16 classes...\n",
      "New structure created.\n"
     ]
    }
   ],
   "source": [
    "# target class\n",
    "CLASSES_TO_CREATE = [\n",
    "    'boot', 'dress', 'pants', 'shirt', 'sneaker', 'flip-flop', 'loafer',\n",
    "    'short', 'skirt', 'slipper', 't-shirt', 'blazer', 'hoodie', 'jacket', 'sweater', 'polo'\n",
    "]\n",
    "\n",
    "print(f\"Creating new directory structure for {len(CLASSES_TO_CREATE)} classes...\")\n",
    "for split in [TRAIN_DIR, VAL_DIR]:\n",
    "    for cls_name in CLASSES_TO_CREATE:\n",
    "        os.makedirs(split / cls_name, exist_ok=True)\n",
    "\n",
    "print(\"New structure created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: \"Re-label\" by Moving Files\n",
    "\n",
    "This is the key step. We use terminal commands (`mv`) to move all images from the Fashion-MNIST folders (e.g., `T-shirt/top`) into your new folders (e.g., `shirt`).\n",
    "\n",
    "This works because we are in the `notebooks/` directory, so `../fashion-mnist` points to the unzipped folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc46a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping new Kaggle datasets and splitting into train/val...\n",
      "\n",
      "Processing Clothes Dataset from: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/clothes-dataset-unzipped\n",
      "  [Warning] Source folder not found: Blazer\n",
      "  [Warning] Source folder not found: Celana_Panjang\n",
      "  [Warning] Source folder not found: Celana_Pendek\n",
      "  [Warning] Source folder not found: Gaun\n",
      "  [Warning] Source folder not found: Hoodie\n",
      "  [Warning] Source folder not found: Jaket\n",
      "  [Warning] Source folder not found: Jaket_Denim\n",
      "  [Warning] Source folder not found: Jaket_Olahraga\n",
      "  [Warning] Source folder not found: Jeans\n",
      "  [Warning] Source folder not found: Kaos\n",
      "  [Warning] Source folder not found: Kemeja\n",
      "  [Warning] Source folder not found: Mantel\n",
      "  [Warning] Source folder not found: Polo\n",
      "  [Warning] Source folder not found: Rok\n",
      "  [Warning] Source folder not found: Sweter\n",
      "\n",
      "Processing Shoe Dataset from: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/shoe-dataset-unzipped\n",
      "  [Warning] Source folder not found: boots\n",
      "  [Warning] Source folder not found: sneakers\n",
      "  [Warning] Source folder not found: flip flops\n",
      "  [Warning] Source folder not found: loafers\n",
      "  [Warning] Source folder not found: sandals\n",
      "  [Warning] Source folder not found: soccer shoes\n",
      "\n",
      "--- File Move Complete ---\n",
      "Total Train Images: 0\n",
      "Total Val Images: 0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Mapping new Kaggle datasets and splitting into train/val...\")\n",
    "\n",
    "# --- Mappings from Kaggle source folders to your 16 target class names ---\n",
    "\n",
    "# Target classes (as defined in the cell above)\n",
    "TARGET_CLASSES = set(CLASSES_TO_CREATE)\n",
    "\n",
    "# 1. ryanbadai/clothes-dataset\n",
    "CLOTHES_BASE_DIR = CLOTHES_SOURCE_DIR\n",
    "\n",
    "CLOTHES_MAP = {\n",
    "    \"Blazer\": \"blazer\",\n",
    "    \"Celana_Panjang\": \"pants\",\n",
    "    \"Celana_Pendek\": \"short\",\n",
    "    \"Gaun\": \"dress\",\n",
    "    \"Hoodie\": \"hoodie\",\n",
    "    \"Jaket\": \"jacket\",\n",
    "    \"Jaket_Denim\": \"jacket\",       # Grouped into 'jacket'\n",
    "    \"Jaket_Olahraga\": \"jacket\",    # Grouped into 'jacket'\n",
    "    \"Jeans\": \"pants\",          # Grouped into 'pants'\n",
    "    \"Kaos\": \"t-shirt\",\n",
    "    \"Kemeja\": \"shirt\",\n",
    "    \"Mantel\": \"jacket\",        # Grouped into 'jacket'\n",
    "    \"Polo\": \"polo\",\n",
    "    \"Rok\": \"skirt\",\n",
    "    \"Sweter\": \"sweater\",\n",
    "}\n",
    "\n",
    "# 2. noobyogi0100/shoe-dataset\n",
    "SHOES_BASE_DIR = SHOES_SOURCE_DIR\n",
    "\n",
    "SHOES_MAP = {\n",
    "    \"boots\": \"boot\",\n",
    "    \"sneakers\": \"sneaker\",\n",
    "    \"flip flops\": \"flip-flop\",\n",
    "    \"loafers\": \"loafer\",\n",
    "    \"sandals\": \"slipper\",      # Mapped to 'slipper'\n",
    "    \"soccer shoes\": \"sneaker\",     # Grouped into 'sneaker'\n",
    "}\n",
    "# --- End Mappings ---\n",
    "\n",
    "\n",
    "def split_and_copy_files(source_class_dir, target_class_name, train_dir, val_dir, split_ratio=0.8):\n",
    "    \"\"\"Copies files from source dir to train/val dirs with a split.\"\"\"\n",
    "    \n",
    "    if target_class_name not in TARGET_CLASSES:\n",
    "        print(f\"  [Skipping] Source '{source_class_dir.name}' maps to '{target_class_name}', which is not in TARGET_CLASSES.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Find all images (jpg, png, jpeg)\n",
    "    images = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "        images.extend(glob.glob(str(source_class_dir / ext)))\n",
    "    \n",
    "    if not images:\n",
    "        print(f\"  [Warning] No images found in {source_class_dir}\")\n",
    "        return 0, 0\n",
    "\n",
    "    random.seed(42) # for reproducible splits\n",
    "    random.shuffle(images)\n",
    "    split_point = int(len(images) * split_ratio)\n",
    "    train_files = images[:split_point]\n",
    "    val_files = images[split_point:]\n",
    "\n",
    "    # Get target directories\n",
    "    target_train_dir = train_dir / target_class_name\n",
    "    target_val_dir = val_dir / target_class_name\n",
    "\n",
    "    # Copy files\n",
    "    for f in train_files:\n",
    "        shutil.copy(f, target_train_dir / Path(f).name)\n",
    "    for f in val_files:\n",
    "        shutil.copy(f, target_val_dir / Path(f).name)\n",
    "        \n",
    "    return len(train_files), len(val_files)\n",
    "\n",
    "total_train = 0\n",
    "total_val = 0\n",
    "\n",
    "# --- Process Clothes Dataset ---\n",
    "print(f\"\\nProcessing Clothes Dataset from: {CLOTHES_BASE_DIR}\")\n",
    "for source_name, target_name in CLOTHES_MAP.items():\n",
    "    source_dir = CLOTHES_BASE_DIR / source_name\n",
    "    # Check for nested \"Clothes_Dataset\" folder\n",
    "    if not source_dir.exists():\n",
    "        nested_dir = CLOTHES_BASE_DIR / \"Clothes_Dataset\" / source_name\n",
    "        if nested_dir.exists():\n",
    "            source_dir = nested_dir\n",
    "        else:\n",
    "            print(f\"  [Warning] Source folder not found: {source_dir.name}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"Processing '{source_name}' -> '{target_name}'...\")\n",
    "    n_train, n_val = split_and_copy_files(source_dir, target_name, TRAIN_DIR, VAL_DIR)\n",
    "    print(f\"  Copied {n_train} train, {n_val} val files.\")\n",
    "    total_train += n_train\n",
    "    total_val += n_val\n",
    "\n",
    "# --- Process Shoe Dataset ---\n",
    "print(f\"\\nProcessing Shoe Dataset from: {SHOES_BASE_DIR}\")\n",
    "for source_name, target_name in SHOES_MAP.items():\n",
    "    source_dir = SHOES_BASE_DIR / source_name\n",
    "    if not source_dir.exists():\n",
    "        print(f\"  [Warning] Source folder not found: {source_dir.name}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing '{source_name}' -> '{target_name}'...\")\n",
    "    n_train, n_val = split_and_copy_files(source_dir, target_name, TRAIN_DIR, VAL_DIR)\n",
    "    print(f\"  Copied {n_train} train, {n_val} val files.\")\n",
    "    total_train += n_train\n",
    "    total_val += n_val\n",
    "\n",
    "print(\"\\n--- File Move Complete ---\")\n",
    "print(f\"Total Train Images: {total_train}\")\n",
    "print(f\"Total Val Images: {total_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: IMPORTANT - Check for Missing Classes\n",
    "\n",
    "The Fashion-MNIST dataset **does not contain** images for `shorts`, `skirt`, or `slipper`.\n",
    "\n",
    "Your folders for these classes are **empty**! You will need to find images for these classes and add them to the `train/` and `val/` subfolders yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9a5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5d: Oversample imbalanced classes in TRAIN_DIR (skirt, short/shorts)\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "TRAIN_DIR = pathlib.Path(TRAIN_DIR)  # ensure Path\n",
    "random.seed(42)\n",
    "\n",
    "TARGETS = [\n",
    "    'boot', 'dress', 'pants', 'shirt', 'sneaker', 'flip-flop', 'loafer',\n",
    "    'short', 'skirt', 'slipper', 't-shirt', 'blazer', 'hoodie', 'sweater', 'polo'\n",
    "]\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "# Optional PIL-based augmentation\n",
    "try:\n",
    "    from PIL import Image, ImageEnhance, ImageOps\n",
    "    HAVE_PIL = True\n",
    "except Exception:\n",
    "    HAVE_PIL = False\n",
    "\n",
    "def _unique_path(dirpath: pathlib.Path, stem: str, ext: str) -> pathlib.Path:\n",
    "    i = 0\n",
    "    while True:\n",
    "        name = f\"{stem}_aug{'' if i==0 else f'_{i}'}{ext}\"\n",
    "        out = dirpath / name\n",
    "        if not out.exists():\n",
    "            return out\n",
    "        i += 1\n",
    "\n",
    "def _list_images(dirpath: pathlib.Path):\n",
    "    return [p for p in dirpath.glob(\"*\") if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]\n",
    "\n",
    "def _pil_augment(img: \"Image.Image\") -> \"Image.Image\":\n",
    "    # simple, fast, safe transforms\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageOps.mirror(img)\n",
    "    angle = random.uniform(-12, 12)\n",
    "    img = img.rotate(angle, resample=Image.BICUBIC, expand=False, fillcolor=(255, 255, 255))\n",
    "    # slight brightness/contrast jitter\n",
    "    img = ImageEnhance.Brightness(img).enhance(random.uniform(0.9, 1.1))\n",
    "    img = ImageEnhance.Contrast(img).enhance(random.uniform(0.9, 1.1))\n",
    "    return img\n",
    "\n",
    "def _augment_or_copy(src: pathlib.Path, dst: pathlib.Path):\n",
    "    if HAVE_PIL:\n",
    "        try:\n",
    "            with Image.open(src) as im:\n",
    "                if im.mode not in (\"RGB\", \"L\"):\n",
    "                    im = im.convert(\"RGB\")\n",
    "                # Save as same ext if supported; else default to .jpg\n",
    "                ext = dst.suffix.lower()\n",
    "                im = _pil_augment(im)\n",
    "                params = {}\n",
    "                if ext in {\".jpg\", \".jpeg\"}:\n",
    "                    params = {\"quality\": 90, \"optimize\": True}\n",
    "                im.save(dst, **params)\n",
    "                return\n",
    "        except Exception as _:\n",
    "            pass\n",
    "    # Fallback to raw copy\n",
    "    shutil.copy2(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fafece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train counts:\n",
      " - blazer    : 500\n",
      " - boot      : 249\n",
      " - dress     : 500\n",
      " - flip-flop : 249\n",
      " - hoodie    : 500\n",
      " - jacket    : 1995\n",
      " - loafer    : 249\n",
      " - pants     : 1000\n",
      " - polo      : 500\n",
      " - shirt     : 500\n",
      " - short     : 500\n",
      " - skirt     : 709\n",
      " - slipper   : 249\n",
      " - sneaker   : 478\n",
      " - sweater   : 500\n",
      " - t-shirt   : 500\n",
      "\n",
      "Oversampling to target count = 1995\n",
      " - boot: adding 1746 samples...\n",
      " - dress: adding 1495 samples...\n",
      " - pants: adding 995 samples...\n",
      " - shirt: adding 1495 samples...\n",
      " - sneaker: adding 1517 samples...\n",
      " - flip-flop: adding 1746 samples...\n",
      " - loafer: adding 1746 samples...\n",
      " - short: adding 1495 samples...\n",
      " - skirt: adding 1286 samples...\n",
      " - slipper: adding 1746 samples...\n",
      " - t-shirt: adding 1495 samples...\n",
      " - blazer: adding 1495 samples...\n",
      " - hoodie: adding 1495 samples...\n",
      " - sweater: adding 1495 samples...\n",
      " - polo: adding 1495 samples...\n",
      "\n",
      "Train counts after oversampling:\n",
      " - blazer    : 1995\n",
      " - boot      : 1995\n",
      " - dress     : 1995\n",
      " - flip-flop : 1995\n",
      " - hoodie    : 1995\n",
      " - jacket    : 1995\n",
      " - loafer    : 1995\n",
      " - pants     : 1995\n",
      " - polo      : 1995\n",
      " - shirt     : 1995\n",
      " - short     : 1995\n",
      " - skirt     : 1995\n",
      " - slipper   : 1995\n",
      " - sneaker   : 1995\n",
      " - sweater   : 1995\n",
      " - t-shirt   : 1995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count current train images per class (based on actual folders)\n",
    "class_dirs = [d for d in TRAIN_DIR.iterdir() if d.is_dir()]\n",
    "counts = {d.name: len(_list_images(d)) for d in class_dirs}\n",
    "print(\"Current train counts:\")\n",
    "for k in sorted(counts):\n",
    "    print(f\" - {k:10s}: {counts[k]}\")\n",
    "\n",
    "if not counts:\n",
    "    raise RuntimeError(\"No training classes found in TRAIN_DIR.\")\n",
    "\n",
    "target_count = max(counts.values())\n",
    "targets_existing = [c for c in TARGETS if (TRAIN_DIR / c).exists()]\n",
    "if not targets_existing:\n",
    "    print(\"No target classes (skirt/short/shorts) found in TRAIN_DIR. Skipping oversampling.\")\n",
    "else:\n",
    "    print(f\"\\nOversampling to target count = {target_count}\")\n",
    "    for cls in targets_existing:\n",
    "        cls_dir = TRAIN_DIR / cls\n",
    "        imgs = _list_images(cls_dir)\n",
    "        n = len(imgs)\n",
    "        if n == 0:\n",
    "            print(f\" - {cls}: no images to oversample from; add some first.\")\n",
    "            continue\n",
    "        need = target_count - n\n",
    "        if need <= 0:\n",
    "            print(f\" - {cls}: already >= target ({n} >= {target_count}).\")\n",
    "            continue\n",
    "\n",
    "        print(f\" - {cls}: adding {need} samples...\")\n",
    "        i = 0\n",
    "        while i < need:\n",
    "            src = random.choice(imgs)\n",
    "            stem = src.stem\n",
    "            ext = src.suffix.lower()\n",
    "            if ext not in ALLOWED_EXTS:\n",
    "                ext = \".jpg\"\n",
    "            dst = _unique_path(cls_dir, stem=stem, ext=ext)\n",
    "            _augment_or_copy(src, dst)\n",
    "            i += 1\n",
    "\n",
    "# Re-count after oversampling\n",
    "counts_after = {d.name: len(_list_images(d)) for d in class_dirs}\n",
    "print(\"\\nTrain counts after oversampling:\")\n",
    "for k in sorted(counts_after):\n",
    "    print(f\" - {k:10s}: {counts_after[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade6a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (train):\n",
      " - boot    : 1636\n",
      " - dress   : 1596\n",
      " - pants   : 1596\n",
      " - shirt   : 1596\n",
      " - sneaker : 1673\n",
      " - flip-flop: 1636\n",
      " - loafer  : 1636\n",
      " - short   : 1596\n",
      " - skirt   : 1596\n",
      " - slipper : 1636\n",
      " - t-shirt : 1596\n",
      " - blazer  : 1596\n",
      " - hoodie  : 1596\n",
      " - jacket  : 1596\n",
      " - sweater : 1596\n",
      " - polo    : 1596\n",
      "\n",
      "Class counts (val):\n",
      " - boot    : 409\n",
      " - dress   : 399\n",
      " - pants   : 399\n",
      " - shirt   : 399\n",
      " - sneaker : 418\n",
      " - flip-flop: 409\n",
      " - loafer  : 409\n",
      " - short   : 399\n",
      " - skirt   : 399\n",
      " - slipper : 409\n",
      " - t-shirt : 399\n",
      " - blazer  : 399\n",
      " - hoodie  : 399\n",
      " - jacket  : 399\n",
      " - sweater : 399\n",
      " - polo    : 399\n"
     ]
    }
   ],
   "source": [
    "# Count per-class files in train/ and val/\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_files(dirpath: pathlib.Path) -> dict:\n",
    "    counts = {}\n",
    "    for cls in CLASSES_TO_CREATE:\n",
    "        d = dirpath / cls\n",
    "        n = len([f for f in d.glob(\"*\") if f.is_file()]) if d.exists() else 0\n",
    "        counts[cls] = n\n",
    "    return counts\n",
    "\n",
    "train_counts = count_files(TRAIN_DIR)\n",
    "val_counts = count_files(VAL_DIR)\n",
    "\n",
    "print(\"Class counts (train):\")\n",
    "for k, v in train_counts.items():\n",
    "    print(f\" - {k:8s}: {v}\")\n",
    "\n",
    "print(\"\\nClass counts (val):\")\n",
    "for k, v in val_counts.items():\n",
    "    print(f\" - {k:8s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5de23",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d497f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n",
      "Val classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset sanity print: list classes in train/ and val/ (no syncing/moving) ---\n",
    "from pathlib import Path\n",
    "\n",
    "train_dir = TRAIN_DIR\n",
    "val_dir = VAL_DIR\n",
    "\n",
    "train_classes = sorted([d.name for d in Path(train_dir).iterdir() if d.is_dir()])\n",
    "val_classes = sorted([d.name for d in Path(val_dir).iterdir() if d.is_dir()])\n",
    "\n",
    "print(\"Train classes:\", train_classes)\n",
    "print(\"Val classes:\", val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0736f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n",
      "Fixed empty splits: none\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "def list_images(dirpath: Path):\n",
    "    return [p for p in dirpath.glob(\"*\") if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]\n",
    "\n",
    "train_dir = Path(TRAIN_DIR)\n",
    "val_dir = Path(VAL_DIR)\n",
    "\n",
    "train_classes = {d.name for d in train_dir.iterdir() if d.is_dir()}\n",
    "val_classes = {d.name for d in val_dir.iterdir() if d.is_dir()}\n",
    "all_classes = sorted(train_classes | val_classes)\n",
    "\n",
    "# Make sure each class exists in both splits\n",
    "for cls in all_classes:\n",
    "    (train_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure at least 1 image per class per split by borrowing from the other split if needed\n",
    "fixed = []\n",
    "for cls in all_classes:\n",
    "    tr_imgs = list_images(train_dir / cls)\n",
    "    va_imgs = list_images(val_dir / cls)\n",
    "\n",
    "    if len(tr_imgs) == 0 and len(va_imgs) > 0:\n",
    "        src = random.choice(va_imgs)\n",
    "        dst = (train_dir / cls) / src.name\n",
    "        c = 1\n",
    "        while dst.exists():\n",
    "            dst = dst.with_name(f\"{src.stem}_{c}{src.suffix}\")\n",
    "            c += 1\n",
    "        shutil.copy2(src, dst)\n",
    "        fixed.append(f\"train/{cls}\")\n",
    "\n",
    "    if len(va_imgs) == 0 and len(tr_imgs) > 0:\n",
    "        src = random.choice(tr_imgs)\n",
    "        dst = (val_dir / cls) / src.name\n",
    "        c = 1\n",
    "        while dst.exists():\n",
    "            dst = dst.with_name(f\"{src.stem}_{c}{src.suffix}\")\n",
    "            c += 1\n",
    "        shutil.copy2(src, dst)\n",
    "        fixed.append(f\"val/{cls}\")\n",
    "\n",
    "print(\"Classes:\", all_classes)\n",
    "print(\"Fixed empty splits:\", fixed if fixed else \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0246acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-epoch reporting WITHOUT touching DEFAULT_CFG (prevents YAML RepresenterError) ---\n",
    "from pathlib import Path\n",
    "\n",
    "REPORT_CSV = PROJECT_ROOT / \"runs-cls\" / \"epoch_report.csv\"\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    csv_path = Path(trainer.save_dir) / \"results.csv\"\n",
    "    if not csv_path.exists():\n",
    "        return\n",
    "    last = csv_path.read_text().strip().splitlines()[-1]\n",
    "    print(f\"[epoch {trainer.epoch + 1}] {last}\")\n",
    "    if not REPORT_CSV.exists():\n",
    "        header = csv_path.read_text().splitlines()[0]\n",
    "        REPORT_CSV.write_text(header + \"\\n\")\n",
    "    with REPORT_CSV.open(\"a\") as f:\n",
    "        f.write(last + \"\\n\")\n",
    "\n",
    "def register_callbacks(model):\n",
    "    # avoid duplicate registration if re-running cells\n",
    "    try:\n",
    "        model.remove_callback(\"on_fit_epoch_end\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.add_callback(\"on_fit_epoch_end\", on_fit_epoch_end)\n",
    "    print(\"Per-epoch report callback registered. Writing mirror CSV to:\", REPORT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a263e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 1.6MB/s 3.4s.3s<0.0s2s8.8ss\n",
      "Per-epoch report callback registered. Writing mirror CSV to: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/epoch_report.csv\n",
      "Ultralytics 8.3.223 üöÄ Python-3.11.14 torch-2.9.0 MPS (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n-cls-fashion, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... found 25772 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... found 6443 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=16\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    350736  ultralytics.nn.modules.head.Classify         [256, 16]                     \n",
      "YOLOv8n-cls summary: 56 layers, 1,458,784 parameters, 1,458,784 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 373.9¬±56.6 MB/s, size: 109.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... 25772 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25772/25772 5.7Kit/s 4.5s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train/boot/image117.jpeg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 452.5¬±80.5 MB/s, size: 112.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... 6443 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6443/6443 6.6Kit/s 1.0s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val/slipper/image288.jpeg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       1/30      1.11G      1.923         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:55<1.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.3sss\n",
      "                   all       0.68      0.966\n",
      "[epoch 1] 1,417.59,1.92332,0.68043,0.96616,1.00045,0.00332506,0.00332506,0.00332506\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       2/30      2.12G     0.9254         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:54<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.793      0.983\n",
      "[epoch 2] 2,834.239,0.92545,0.79311,0.98293,0.6287,0.00643867,0.00643867,0.00643867\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       3/30      1.12G     0.6459         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:51<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.828      0.992\n",
      "[epoch 3] 3,1307.49,0.64594,0.82772,0.99239,0.48668,0.00933227,0.00933227,0.00933227\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       4/30      2.11G     0.5072         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.9it/s 7:43<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.865      0.993\n",
      "[epoch 4] 4,1833.43,0.50724,0.86512,0.99317,0.39393,0.00901,0.00901,0.00901\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       5/30      2.12G     0.4008         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:16<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.898      0.996\n",
      "[epoch 5] 5,2272.76,0.40076,0.89787,0.99643,0.30091,0.00868,0.00868,0.00868\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       6/30      2.12G     0.3371         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:57<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.906      0.997\n",
      "[epoch 6] 6,2693.58,0.33712,0.90579,0.99659,0.2867,0.00835,0.00835,0.00835\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       7/30      2.12G     0.2966         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:52<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.9it/s 58.9s1.2ss\n",
      "                   all      0.899      0.996\n",
      "[epoch 7] 7,3104.7,0.29661,0.89912,0.99596,0.30596,0.00802,0.00802,0.00802\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       8/30      2.12G       0.26         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:59<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.2sss\n",
      "                   all      0.917      0.997\n",
      "[epoch 8] 8,3587.5,0.25999,0.91681,0.99736,0.2554,0.00769,0.00769,0.00769\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       9/30      2.12G     0.2365         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:31<0.9s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:001.2sss\n",
      "                   all      0.918      0.998\n",
      "[epoch 9] 9,4039.2,0.23651,0.91774,0.99752,0.25006,0.00736,0.00736,0.00736\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      10/30      2.12G     0.2156         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 7:01<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.1sss\n",
      "                   all      0.931      0.998\n",
      "[epoch 10] 10,4523.32,0.21559,0.93093,0.99798,0.21408,0.00703,0.00703,0.00703\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      11/30      2.12G     0.2079         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:33<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:051.2sss\n",
      "                   all      0.937      0.999\n",
      "[epoch 11] 11,4981.78,0.20789,0.93699,0.9986,0.19803,0.0067,0.0067,0.0067\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      12/30      2.12G     0.1891         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:53<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:011.2sss\n",
      "                   all      0.951      0.999\n",
      "[epoch 12] 12,5396.06,0.18906,0.95064,0.99876,0.15965,0.00637,0.00637,0.00637\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      13/30      2.11G     0.1724         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:57<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.7it/s 1:121.3sss\n",
      "                   all       0.95      0.998\n",
      "[epoch 13] 13,5826.07,0.17239,0.94956,0.99798,0.17469,0.00604,0.00604,0.00604\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      14/30      2.12G      0.167         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:11<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.956      0.998\n",
      "[epoch 14] 14,6259.67,0.16696,0.95592,0.99814,0.1443,0.00571,0.00571,0.00571\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      15/30      2.12G      0.151         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.2it/s 5:46<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.956      0.999\n",
      "[epoch 15] 15,6668.22,0.15096,0.95577,0.9986,0.14368,0.00538,0.00538,0.00538\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      16/30      2.12G     0.1424         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:51<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:051.2sss\n",
      "                   all      0.956      0.999\n",
      "[epoch 16] 16,7144.7,0.14244,0.95608,0.99907,0.14841,0.00505,0.00505,0.00505\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      17/30      2.12G      0.138         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:30<1.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.6it/s 1:301.8sss\n",
      "                   all      0.962      0.999\n",
      "[epoch 17] 17,7625.75,0.13798,0.96197,0.99891,0.1296,0.00472,0.00472,0.00472\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      18/30      2.11G     0.1294         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:47<0.9s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.7it/s 1:081.9sss\n",
      "                   all      0.962      0.999\n",
      "[epoch 18] 18,8102.08,0.12938,0.96244,0.99922,0.12479,0.00439,0.00439,0.00439\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      19/30      2.12G     0.1187         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:17<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.964      0.999\n",
      "[epoch 19] 19,8542.49,0.11866,0.96399,0.99876,0.12334,0.00406,0.00406,0.00406\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      20/30      2.11G     0.1139         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 5:57<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.965      0.999\n",
      "[epoch 20] 20,8961.73,0.11394,0.96492,0.99907,0.11871,0.00373,0.00373,0.00373\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      21/30      2.11G     0.1054         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.8it/s 8:00<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.964      0.999\n",
      "[epoch 21] 21,9504.29,0.10538,0.9643,0.99938,0.11956,0.0034,0.0034,0.0034\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      22/30      2.11G    0.09886         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:28<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:031.3sss\n",
      "                   all      0.967      0.999\n",
      "[epoch 22] 22,9956.06,0.09886,0.96694,0.99922,0.1179,0.00307,0.00307,0.00307\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      23/30      2.12G    0.09531         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.8it/s 8:18<1.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.6it/s 1:311.1sss\n",
      "                   all      0.967      0.999\n",
      "[epoch 23] 23,10546.1,0.09531,0.9671,0.99922,0.11103,0.00274,0.00274,0.00274\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      24/30      2.12G    0.09017         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:26<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.5it/s 1:331.8sss\n",
      "                   all      0.966      0.999\n",
      "[epoch 24] 24,11025.9,0.09017,0.96554,0.99907,0.10563,0.00241,0.00241,0.00241\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      25/30      2.12G    0.08785         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:38<1.2s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.6it/s 1:291.7sss\n",
      "                   all      0.968      0.999\n",
      "[epoch 25] 25,11513.7,0.08785,0.96803,0.99907,0.10498,0.00208,0.00208,0.00208\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      26/30      2.12G    0.08333         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.9it/s 7:05<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.2sss\n",
      "                   all      0.968      0.999\n",
      "[epoch 26] 26,12001.7,0.08333,0.96849,0.99907,0.10469,0.00175,0.00175,0.00175\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      27/30      2.11G    0.08012         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.0it/s 6:56<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:071.6sss\n",
      "                   all      0.968      0.999\n",
      "[epoch 27] 27,12485.2,0.08012,0.96834,0.99922,0.10198,0.00142,0.00142,0.00142\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      28/30      2.11G    0.07533         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 0.9it/s 7:36<0.8s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:051.2sss\n",
      "                   all      0.969      0.999\n",
      "[epoch 28] 28,13006.5,0.07533,0.9688,0.99922,0.10023,0.00109,0.00109,0.00109\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      29/30      2.11G    0.07337         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:08<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:021.3sss\n",
      "                   all       0.97      0.999\n",
      "[epoch 29] 29,13437.7,0.07337,0.9702,0.99907,0.09796,0.00076,0.00076,0.00076\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      30/30      2.12G    0.07053         44        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 403/403 1.1it/s 6:02<0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:071.7sss\n",
      "                   all      0.969      0.999\n",
      "[epoch 30] 30,13867.7,0.07053,0.96896,0.99891,0.09895,0.00043,0.00043,0.00043\n",
      "\n",
      "30 epochs completed in 3.852 hours.\n",
      "Optimizer stripped from /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/last.pt, 3.0MB\n",
      "Optimizer stripped from /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt...\n",
      "Ultralytics 8.3.223 üöÄ Python-3.11.14 torch-2.9.0 MPS (Apple M4)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,455,376 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/train... found 25772 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch/val... found 6443 images in 16 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 0.8it/s 1:061.2sss\n",
      "                   all       0.97      0.999\n",
      "Speed: 0.1ms preprocess, 0.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "[epoch 30] 30,13867.7,0.07053,0.96896,0.99891,0.09895,0.00043,0.00043,0.00043\n",
      "Best weights: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Train YOLOv8n-cls (MPS if available) ---\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Remove any default callbacks so the 'callbacks' key doesn't leak into args.yaml/cfg\n",
    "try:\n",
    "    if hasattr(DEFAULT_CFG, \"callbacks\"):\n",
    "        delattr(DEFAULT_CFG, \"callbacks\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 4\n",
    "BATCH = 64\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "model = YOLO(\"yolov8n-cls.pt\")\n",
    "\n",
    "# Extra safety: strip any 'callbacks' from model overrides if present\n",
    "try:\n",
    "    if hasattr(model, \"overrides\") and isinstance(model.overrides, dict):\n",
    "        model.overrides.pop(\"callbacks\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Register console/CSV epoch reporter on the model only (not in DEFAULT_CFG)\n",
    "try:\n",
    "    register_callbacks(model)\n",
    "except NameError:\n",
    "    REPORT_CSV = PROJECT_ROOT / \"runs-cls\" / \"epoch_report.csv\"\n",
    "    def _on_fit_epoch_end(trainer):\n",
    "        csv_path = Path(trainer.save_dir) / \"results.csv\"\n",
    "        if not csv_path.exists():\n",
    "            return\n",
    "        last = csv_path.read_text().strip().splitlines()[-1]\n",
    "        print(f\"[epoch {trainer.epoch + 1}] {last}\")\n",
    "        if not REPORT_CSV.exists():\n",
    "            header = csv_path.read_text().splitlines()[0]\n",
    "            REPORT_CSV.write_text(header + \"\\n\")\n",
    "        with REPORT_CSV.open(\"a\") as f:\n",
    "            f.write(last + \"\\n\")\n",
    "    try:\n",
    "        model.remove_callback(\"on_fit_epoch_end\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.add_callback(\"on_fit_epoch_end\", _on_fit_epoch_end)\n",
    "    print(\"Per-epoch report callback registered. Writing mirror CSV to:\", REPORT_CSV)\n",
    "\n",
    "RUNS_DIR = PROJECT_ROOT / \"runs-cls\"\n",
    "NAME = \"yolov8n-cls-fashion\"\n",
    "EPOCHS = 30\n",
    "IMGSZ = 224\n",
    "SEED = 42\n",
    "\n",
    "results = model.train(\n",
    "    data=str(DATASET_DIR),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=NAME,\n",
    "    seed=SEED,\n",
    "    patience=10,         # early stopping\n",
    "    verbose=True,\n",
    "    device=DEVICE,\n",
    "    workers=NUM_WORKERS,\n",
    "    plots=False,\n",
    ")\n",
    "\n",
    "best_path = RUNS_DIR / NAME / \"weights\" / \"best.pt\"\n",
    "print(\"Best weights:\", best_path if best_path.exists() else \"see run dir for weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f476ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m metrics = \u001b[43mmodel\u001b[49m.val(\n\u001b[32m      2\u001b[39m     data=\u001b[38;5;28mstr\u001b[39m(DATASET_DIR),\n\u001b[32m      3\u001b[39m     imgsz=IMGSZ,\n\u001b[32m      4\u001b[39m     batch=BATCH,\n\u001b[32m      5\u001b[39m     plots=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m     project=\u001b[38;5;28mstr\u001b[39m(RUNS_DIR),\n\u001b[32m      7\u001b[39m     name=NAME + \u001b[33m\"\u001b[39m\u001b[33m-val\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtop1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.top1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, top5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.top5\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = model.val(\n",
    "    data=str(DATASET_DIR),\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    plots=True,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=NAME + \"-val\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"top1: {metrics.top1:.4f}, top5: {metrics.top5:.4f}\")\n",
    "except Exception:\n",
    "    print(\"Validation metrics:\", metrics)\n",
    "print(\"Val plots saved to:\", RUNS_DIR / (NAME + \"-val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8355b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime 1.23.2 CPUExecutionProvider\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "input shape last dimension expected 4 but input shape is torch.Size([16, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m IMGSZ = \u001b[32m224\u001b[39m\n\u001b[32m      6\u001b[39m model = YOLO(best_path)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTEST_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMGSZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRUNS_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-pred\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved predictions to:\u001b[39m\u001b[33m\"\u001b[39m, RUNS_DIR / (NAME + \u001b[33m\"\u001b[39m\u001b[33m-pred\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:557\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:230\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:344\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m2\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28mself\u001b[39m.run_callbacks(\u001b[33m\"\u001b[39m\u001b[33mon_predict_postprocess_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/ultralytics/models/yolo/detect/predict.py:56\u001b[39m, in \u001b[36mDetectionPredictor.postprocess\u001b[39m\u001b[34m(self, preds, img, orig_imgs, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03mPost-process predictions and return a list of Results objects.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m \u001b[33;03m    >>> processed_results = predictor.postprocess(preds, img, orig_imgs)\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     55\u001b[39m save_feats = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_feats\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m preds = \u001b[43mnms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnon_max_suppression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43miou\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43magnostic_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetect\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend2end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend2end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrotated\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[32m     70\u001b[39m     orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/ultralytics/utils/nms.py:87\u001b[39m, in \u001b[36mnon_max_suppression\u001b[39m\u001b[34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, rotated, end2end, return_idxs)\u001b[39m\n\u001b[32m     85\u001b[39m prediction = prediction.transpose(-\u001b[32m1\u001b[39m, -\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# shape(1,84,6300) to shape(1,6300,84)\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rotated:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     prediction[..., :\u001b[32m4\u001b[39m] = \u001b[43mxywh2xyxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# xywh to xyxy\u001b[39;00m\n\u001b[32m     89\u001b[39m t = time.time()\n\u001b[32m     90\u001b[39m output = [torch.zeros((\u001b[32m0\u001b[39m, \u001b[32m6\u001b[39m + extra), device=prediction.device)] * bs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/Machine Learning Model/modisch-model-cls/.venv/lib/python3.11/site-packages/ultralytics/utils/ops.py:288\u001b[39m, in \u001b[36mxywh2xyxy\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mxywh2xyxy\u001b[39m(x):\n\u001b[32m    278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m    top-left corner and (x2, y2) is the bottom-right corner. Note: ops per 2 channels faster than per channel.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m \u001b[33;03m        (np.ndarray | torch.Tensor): Bounding box coordinates in (x1, y1, x2, y2) format.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m x.shape[-\u001b[32m1\u001b[39m] == \u001b[32m4\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput shape last dimension expected 4 but input shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m     y = empty_like(x)  \u001b[38;5;66;03m# faster than clone/copy\u001b[39;00m\n\u001b[32m    290\u001b[39m     xy = x[..., :\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# centers\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: input shape last dimension expected 4 but input shape is torch.Size([16, 1])"
     ]
    }
   ],
   "source": [
    "TEST_DIR = PROJECT_ROOT / \"test\"\n",
    "\n",
    "preds = model.predict(\n",
    "    source=str(TEST_DIR),\n",
    "    imgsz=IMGSZ,\n",
    "    save=True,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=NAME + \"-pred\",\n",
    "    verbose=False,\n",
    ")\n",
    "print(\"Saved predictions to:\", RUNS_DIR / (NAME + \"-pred\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.223 üöÄ Python-3.11.14 torch-2.9.0 CPU (Apple M4)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,455,376 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt' with input shape (1, 3, 224, 224) BCHW and output shape(s) (1, 16) (2.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.72...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 0.5s, saved as '/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.onnx' (5.6 MB)\n",
      "\n",
      "Export complete (0.6s)\n",
      "Results saved to \u001b[1m/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.onnx imgsz=224  \n",
      "Validate:        yolo val task=classify model=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.onnx imgsz=224 data=/Users/macm4/repositories/Machine Learning Model/modisch-model-cls/data/dataset-fashion-modisch  \n",
      "Visualize:       https://netron.app\n",
      "Exported ONNX\n"
     ]
    }
   ],
   "source": [
    "RUNS_DIR = PROJECT_ROOT / \"runs-cls\"\n",
    "NAME = \"yolov8n-cls-fashion\"\n",
    "best_path = RUNS_DIR / NAME / \"weights\" / \"best.pt\"\n",
    "IMGSZ = 224\n",
    "model = YOLO(best_path)\n",
    "\n",
    "# Optional exports for deployment\n",
    "try:\n",
    "    model.export(format=\"onnx\", opset=12)\n",
    "    print(\"Exported ONNX\")\n",
    "except Exception as e:\n",
    "    print(\"Export skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f5d41e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ONNX: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.onnx\n",
      "Classes: ['blazer', 'boot', 'dress', 'flip-flop', 'hoodie', 'jacket', 'loafer', 'pants', 'polo', 'shirt', 'short', 'skirt', 'slipper', 'sneaker', 'sweater', 't-shirt']\n",
      "[OK ] blazer       -> blazer        conf=0.150  (blazzer_korea_lilac_1647310416_103c0f39_progressive_thumbnail_aug_1.jpg)\n",
      "[OK ] boot         -> boot          conf=0.153  (image88_aug_2.jpeg)\n",
      "[OK ] dress        -> dress         conf=0.152  (d1dd492a-77b6-4d76-bc22-852cda12c437_aug_4.jpg)\n",
      "[OK ] flip-flop    -> flip-flop     conf=0.153  (image63.jpeg)\n",
      "[OK ] hoodie       -> hoodie        conf=0.153  (hoodie_champion_logo_towel_bro_1696990864_ecdb72d4_progressive_thumbnail_aug.jpg)\n",
      "[OK ] jacket       -> jacket        conf=0.153  (jaket_gunung_outdoor_olahraga__1694095720_70ff8587_progressive_thumbnail.jpg)\n",
      "[OK ] loafer       -> loafer        conf=0.153  (image88_aug_2.jpeg)\n",
      "[OK ] pants        -> pants         conf=0.153  (celana_panjang_pria_semi_jeans_1691767382_61562a6f_progressive_thumbnail.jpg)\n",
      "[OK ] polo         -> polo          conf=0.153  (baju_kaos_polo_sport_bekas_sec_1691759266_bdd3acf6_progressive_thumbnail_aug_3.jpg)\n",
      "[OK ] shirt        -> shirt         conf=0.153  (jual_kemeja_casual_salur_club__1678076338_a96815e2_progressive_thumbnail_aug.jpg)\n",
      "[OK ] short        -> short         conf=0.153  (celana_pendek_shortpants_nike__1696270225_124b0ace_progressive_thumbnail.jpg)\n",
      "[OK ] skirt        -> skirt         conf=0.153  (grey_skirt_emporio_armani_1697017496_65e658bc_progressive_thumbnail_aug.jpg)\n",
      "[OK ] slipper      -> slipper       conf=0.153  (image53_aug_4.jpeg)\n",
      "[OK ] sneaker      -> sneaker       conf=0.153  (image88_aug_2.jpeg)\n",
      "[OK ] sweater      -> sweater       conf=0.153  (d9e84490-185d-48f9-ac16-4ef3360616d5.jpg)\n",
      "[OK ] t-shirt      -> t-shirt       conf=0.153  (648f665e-14d5-4c59-8866-e75f88ae3c8c_aug_5.jpg)\n",
      "\n",
      "Passed 16/16 classes.\n"
     ]
    }
   ],
   "source": [
    "# One-sample-per-class ONNX sanity test (matches Ultralytics preprocessing)\n",
    "\n",
    "import sys, platform, subprocess, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure onnxruntime is available (Apple Silicon uses onnxruntime-silicon)\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "except Exception:\n",
    "    pkg = \"onnxruntime-silicon\" if (sys.platform == \"darwin\" and platform.machine() == \"arm64\") else \"onnxruntime\"\n",
    "    print(f\"Installing {pkg} ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    import onnxruntime as ort\n",
    "\n",
    "# Resolve paths\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parent if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DATASET_DIR = DATA_DIR / \"dataset-fashion-modisch\"\n",
    "VAL_DIR = DATASET_DIR / \"val\"\n",
    "TRAIN_DIR = DATASET_DIR / \"train\"\n",
    "\n",
    "try:\n",
    "    NAME\n",
    "except NameError:\n",
    "    NAME = \"yolov8n-cls-fashion\"\n",
    "\n",
    "onnx_path = PROJECT_ROOT / \"runs-cls\" / NAME / \"weights\" / \"best.onnx\"\n",
    "if not onnx_path.exists():\n",
    "    cand = list((PROJECT_ROOT / \"runs-cls\").rglob(\"best.onnx\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(\"best.onnx not found under runs-cls.\")\n",
    "    onnx_path = cand[0]\n",
    "\n",
    "# Try to get class order from best.pt (training-time names)\n",
    "class_names = None\n",
    "pt_candidate = onnx_path.with_suffix(\".pt\")\n",
    "if not pt_candidate.exists():\n",
    "    # fallback: any best.pt near the runs\n",
    "    pts = list((PROJECT_ROOT / \"runs-cls\").rglob(\"best.pt\"))\n",
    "    if pts:\n",
    "        pt_candidate = pts[0]\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    if pt_candidate and Path(pt_candidate).exists():\n",
    "        names_map = YOLO(str(pt_candidate)).names  # dict[int] -> str\n",
    "        class_names = [names_map[i] for i in sorted(names_map.keys())]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Fallback to dataset folder order if needed\n",
    "if class_names is None:\n",
    "    val_classes = sorted([d.name for d in VAL_DIR.iterdir() if d.is_dir()]) if VAL_DIR.exists() else []\n",
    "    train_classes = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()]) if TRAIN_DIR.exists() else []\n",
    "    class_names = sorted(set(val_classes) | set(train_classes))\n",
    "    if not class_names:\n",
    "        raise RuntimeError(\"No class folders found in train/ or val/.\")\n",
    "\n",
    "print(\"Using ONNX:\", onnx_path)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "def list_images(p: Path):\n",
    "    return [f for f in p.iterdir() if f.is_file() and f.suffix.lower() in ALLOWED_EXTS]\n",
    "\n",
    "IMGSZ = 224\n",
    "\n",
    "# Ultralytics cls preprocessing: resize to IMGSZ and scale to [0,1], no mean/std\n",
    "def preprocess(img_path: Path, size=IMGSZ):\n",
    "    im = Image.open(img_path).convert(\"RGB\").resize((size, size), Image.BILINEAR)\n",
    "    arr = np.asarray(im).astype(np.float32) / 255.0  # HWC in [0,1]\n",
    "    arr = arr.transpose(2, 0, 1)[None, ...]         # NCHW\n",
    "    return arr\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "# Pick one sample per class (prefer val/, else train/)\n",
    "samples = []\n",
    "for cls in class_names:\n",
    "    cand = list_images(VAL_DIR / cls) if (VAL_DIR / cls).exists() else []\n",
    "    if not cand:\n",
    "        cand = list_images(TRAIN_DIR / cls) if (TRAIN_DIR / cls).exists() else []\n",
    "    if cand:\n",
    "        samples.append((cls, cand[0]))\n",
    "if not samples:\n",
    "    raise RuntimeError(\"No images found across splits to test.\")\n",
    "\n",
    "sess = ort.InferenceSession(str(onnx_path), providers=[\"CPUExecutionProvider\"])\n",
    "inp = sess.get_inputs()[0].name\n",
    "out = sess.get_outputs()[0].name\n",
    "\n",
    "ok = 0\n",
    "results = []\n",
    "for true_cls, img_path in samples:\n",
    "    x = preprocess(img_path)\n",
    "    logits = sess.run([out], {inp: x})[0][0]        # (num_classes,)\n",
    "    probs = softmax(logits)\n",
    "    idx = int(np.argmax(probs))\n",
    "    conf = float(probs[idx])\n",
    "    pred_cls = class_names[idx] if idx < len(class_names) else f\"idx_{idx}\"\n",
    "    correct = (pred_cls == true_cls)\n",
    "    ok += int(correct)\n",
    "    results.append((true_cls, img_path.name, pred_cls, conf, correct))\n",
    "\n",
    "for true_cls, fname, pred_cls, conf, correct in results:\n",
    "    mark = \"OK \" if correct else \"ERR\"\n",
    "    print(f\"[{mark}] {true_cls:12s} -> {pred_cls:12s}  conf={conf:.3f}  ({fname})\")\n",
    "\n",
    "print(f\"\\nPassed {ok}/{len(results)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8861ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights: /Users/macm4/repositories/Machine Learning Model/modisch-model-cls/runs-cls/yolov8n-cls-fashion/weights/best.pt\n",
      "Testing 16 classes (one image each) on device=mps ...\n",
      "[OK ] blazer       -> blazer        conf=1.000  (blazzer_korea_lilac_1647310416_103c0f39_progressive_thumbnail_aug_1.jpg)\n",
      "[OK ] boot         -> boot          conf=1.000  (image88_aug_2.jpeg)\n",
      "[OK ] dress        -> dress         conf=1.000  (d1dd492a-77b6-4d76-bc22-852cda12c437_aug_4.jpg)\n",
      "[OK ] flip-flop    -> flip-flop     conf=1.000  (image63.jpeg)\n",
      "[OK ] hoodie       -> hoodie        conf=1.000  (hoodie_champion_logo_towel_bro_1696990864_ecdb72d4_progressive_thumbnail_aug.jpg)\n",
      "[OK ] jacket       -> jacket        conf=1.000  (jaket_gunung_outdoor_olahraga__1694095720_70ff8587_progressive_thumbnail.jpg)\n",
      "[OK ] loafer       -> loafer        conf=1.000  (image88_aug_2.jpeg)\n",
      "[OK ] pants        -> pants         conf=1.000  (celana_panjang_pria_semi_jeans_1691767382_61562a6f_progressive_thumbnail.jpg)\n",
      "[OK ] polo         -> polo          conf=1.000  (baju_kaos_polo_sport_bekas_sec_1691759266_bdd3acf6_progressive_thumbnail_aug_3.jpg)\n",
      "[OK ] shirt        -> shirt         conf=1.000  (jual_kemeja_casual_salur_club__1678076338_a96815e2_progressive_thumbnail_aug.jpg)\n",
      "[OK ] short        -> short         conf=1.000  (celana_pendek_shortpants_nike__1696270225_124b0ace_progressive_thumbnail.jpg)\n",
      "[OK ] skirt        -> skirt         conf=0.998  (grey_skirt_emporio_armani_1697017496_65e658bc_progressive_thumbnail_aug.jpg)\n",
      "[OK ] slipper      -> slipper       conf=1.000  (image53_aug_4.jpeg)\n",
      "[OK ] sneaker      -> sneaker       conf=1.000  (image88_aug_2.jpeg)\n",
      "[OK ] sweater      -> sweater       conf=0.983  (d9e84490-185d-48f9-ac16-4ef3360616d5.jpg)\n",
      "[OK ] t-shirt      -> t-shirt       conf=1.000  (648f665e-14d5-4c59-8866-e75f88ae3c8c_aug_5.jpg)\n",
      "\n",
      "Passed 16/16 classes.\n"
     ]
    }
   ],
   "source": [
    "# One-sample-per-class sanity test using best.pt (Ultralytics YOLO cls)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, platform, random\n",
    "import numpy as np\n",
    "\n",
    "# Deps\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except Exception:\n",
    "    print(\"Installing ultralytics ...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ultralytics\"])\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "except Exception:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "# Resolve project paths\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parent if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DATASET_DIR = DATA_DIR / \"dataset-fashion-modisch\"\n",
    "VAL_DIR = DATASET_DIR / \"val\"\n",
    "TRAIN_DIR = DATASET_DIR / \"train\"\n",
    "\n",
    "# Locate best.pt\n",
    "try:\n",
    "    NAME\n",
    "except NameError:\n",
    "    NAME = \"yolov8n-cls-fashion\"\n",
    "pt_path = PROJECT_ROOT / \"runs-cls\" / NAME / \"weights\" / \"best.pt\"\n",
    "if not pt_path.exists():\n",
    "    # fallback: first best.pt anywhere under runs-cls\n",
    "    candidates = list((PROJECT_ROOT / \"runs-cls\").rglob(\"best.pt\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"best.pt not found under runs-cls. Train/export first or adjust path.\")\n",
    "    pt_path = candidates[0]\n",
    "\n",
    "print(\"Using weights:\", pt_path)\n",
    "\n",
    "# Build class list from folders (alphabetical)\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "def list_images(p: Path):\n",
    "    return [f for f in p.iterdir() if f.is_file() and f.suffix.lower() in ALLOWED_EXTS]\n",
    "\n",
    "val_classes = sorted([d.name for d in VAL_DIR.iterdir() if d.is_dir()]) if VAL_DIR.exists() else []\n",
    "train_classes = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()]) if TRAIN_DIR.exists() else []\n",
    "class_names_ds = sorted(set(val_classes) | set(train_classes))\n",
    "if not class_names_ds:\n",
    "    raise RuntimeError(\"No class folders found in train/ or val/.\")\n",
    "\n",
    "# Pick one sample per class (prefer val, else train)\n",
    "samples = []\n",
    "for cls in class_names_ds:\n",
    "    cand = list_images(VAL_DIR / cls) if (VAL_DIR / cls).exists() else []\n",
    "    if not cand:\n",
    "        cand = list_images(TRAIN_DIR / cls) if (TRAIN_DIR / cls).exists() else []\n",
    "    if cand:\n",
    "        samples.append((cls, cand[0]))  # first image; switch to random.choice(cand) for randomness\n",
    "\n",
    "if not samples:\n",
    "    raise RuntimeError(\"No images found across splits to test.\")\n",
    "\n",
    "# Inference\n",
    "try:\n",
    "    IMGSZ\n",
    "except NameError:\n",
    "    IMGSZ = 224\n",
    "\n",
    "model = YOLO(str(pt_path))\n",
    "# Prefer model.names (training-time class order) for mapping predictions\n",
    "try:\n",
    "    names_map = model.names  # dict[int] -> str\n",
    "    class_names_model = [names_map[i] for i in sorted(names_map.keys())]\n",
    "except Exception:\n",
    "    class_names_model = class_names_ds\n",
    "\n",
    "paths = [p for _, p in samples]\n",
    "print(f\"Testing {len(paths)} classes (one image each) on device={DEVICE} ...\")\n",
    "results = model.predict(source=paths, imgsz=IMGSZ, batch=min(32, len(paths)), device=DEVICE, verbose=False)\n",
    "\n",
    "ok = 0\n",
    "summary = []\n",
    "for (true_cls, img_path), r in zip(samples, results):\n",
    "    probs = getattr(r, \"probs\", None)\n",
    "    if probs is None:\n",
    "        raise RuntimeError(\"Result has no .probs ‚Äî ensure this is a classification model.\")\n",
    "    # index\n",
    "    idx = int(getattr(probs, \"top1\", None)) if hasattr(probs, \"top1\") else int(np.asarray(probs).ravel().argmax())\n",
    "    # confidence\n",
    "    arr = None\n",
    "    if hasattr(probs, \"data\"):\n",
    "        data = probs.data\n",
    "        arr = data.detach().cpu().numpy().ravel() if hasattr(data, \"detach\") else np.asarray(data).ravel()\n",
    "    if arr is None or arr.size == 0:\n",
    "        arr = np.asarray(probs).ravel()\n",
    "    conf = float(arr[idx]) if arr.size else float(\"nan\")\n",
    "    # map index -> name\n",
    "    pred_cls = class_names_model[idx] if idx < len(class_names_model) else f\"idx_{idx}\"\n",
    "    correct = (pred_cls == true_cls)\n",
    "    ok += int(correct)\n",
    "    summary.append((true_cls, img_path.name, pred_cls, conf, correct))\n",
    "\n",
    "# Print summary\n",
    "for true_cls, fname, pred_cls, conf, correct in summary:\n",
    "    mark = \"OK \" if correct else \"ERR\"\n",
    "    print(f\"[{mark}] {true_cls:12s} -> {pred_cls:12s}  conf={conf:.3f}  ({fname})\")\n",
    "\n",
    "print(f\"\\nPassed {ok}/{len(summary)} classes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
